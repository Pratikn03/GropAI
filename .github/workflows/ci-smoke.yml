name: ci-smoke

on:
  push:
    branches:
      - main
  pull_request:

jobs:
  smoke:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install backend dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r backend/requirements.txt
          pip install pytest

      - name: Prepare synthetic dataset for CI
        run: |
          mkdir -p data/raw/tabular configs/tabular reports/explain/tabular/batch
          python - <<'PY'
          from pathlib import Path
          import json
          import numpy as np
          import pandas as pd

          n = 600
          rng = np.random.default_rng(42)
          x1 = rng.normal(0, 1, n)
          x2 = rng.normal(0, 1, n)
          logit = 0.8 * x1 - 0.6 * x2
          p = 1 / (1 + np.exp(-logit))
          y = (p > 0.5).astype(int)
          df = pd.DataFrame({"f1": x1, "f2": x2, "label": y})
          path = Path("data/raw/tabular/data_ci_smoke.csv")
          df.to_csv(path, index=False)
          config = {
              "csv_path": str(path),
              "target": "label",
              "n_trials": 3,
              "seed": 42,
              "max_rows": 300,
              "topk": 8,
          }
          Path("configs/tabular").mkdir(parents=True, exist_ok=True)
          Path("configs/tabular/lightgbm_hpo.yml").write_text(json.dumps(config, indent=2), encoding="utf-8")
          print("Synthetic dataset ready for CI smoke")
          PY

      - name: LightGBM hyper-parameter search (3 trials)
        run: python scripts/tabular/hpo_lightgbm_optuna.py configs/tabular/lightgbm_hpo.yml

      - name: Batch SHAP summaries
        run: python scripts/tabular/shap_batch.py

      - name: Validate artifacts exist
        run: |
          test -f models/tabular/lightgbm_hpo/model_full.joblib
          test -f reports/explain/tabular/batch/global_importance.csv
          test -f reports/explain/tabular/batch/per_sample_topk.csv

      - name: API integration tests (DRY_RUN)
        env:
          DRY_RUN: "1"
        run: |
          pytest backend/app/tests/test_train_integration.py -q
          pytest backend/app/tests/test_rag_integration.py -q
          pytest backend/app/tests/test_explain_integration.py -q
